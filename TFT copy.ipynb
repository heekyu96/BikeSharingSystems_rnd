{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huigyu\\anaconda3\\envs\\BSS\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, Dict, Tuple, Union\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "import optuna.logging\n",
    "import statsmodels.api as sm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "\n",
    "optuna_logger = logging.getLogger(\"optuna\")\n",
    "\n",
    "\n",
    "# need to inherit from callback for this to work\n",
    "class PyTorchLightningPruningCallbackAdjusted(pl.Callback, PyTorchLightningPruningCallback):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def optimize_hyperparameters(\n",
    "    train_dataloaders: DataLoader,\n",
    "    val_dataloaders: DataLoader,\n",
    "    model_path: str,\n",
    "    max_epochs: int = 20,\n",
    "    n_trials: int = 100,\n",
    "    timeout: float = 3600 * 8.0,  # 8 hours\n",
    "    gradient_clip_val_range: Tuple[float, float] = (0.01, 100.0),\n",
    "    hidden_size_range: Tuple[int, int] = (16, 265),\n",
    "    hidden_continuous_size_range: Tuple[int, int] = (8, 64),\n",
    "    attention_head_size_range: Tuple[int, int] = (1, 4),\n",
    "    dropout_range: Tuple[float, float] = (0.1, 0.3),\n",
    "    learning_rate_range: Tuple[float, float] = (1e-5, 1.0),\n",
    "    use_learning_rate_finder: bool = True,\n",
    "    trainer_kwargs: Dict[str, Any] = {},\n",
    "    log_dir: str = \"lightning_logs\",\n",
    "    study: optuna.Study = None,\n",
    "    verbose: Union[int, bool] = None,\n",
    "    pruner: optuna.pruners.BasePruner = optuna.pruners.SuccessiveHalvingPruner(),\n",
    "    **kwargs,\n",
    ") -> optuna.Study:\n",
    "    \"\"\"\n",
    "    Optimize Temporal Fusion Transformer hyperparameters.\n",
    "\n",
    "    Run hyperparameter optimization. Learning rate for is determined with\n",
    "    the PyTorch Lightning learning rate finder.\n",
    "\n",
    "    Args:\n",
    "        train_dataloaders (DataLoader): dataloader for training model\n",
    "        val_dataloaders (DataLoader): dataloader for validating model\n",
    "        model_path (str): folder to which model checkpoints are saved\n",
    "        max_epochs (int, optional): Maximum number of epochs to run training. Defaults to 20.\n",
    "        n_trials (int, optional): Number of hyperparameter trials to run. Defaults to 100.\n",
    "        timeout (float, optional): Time in seconds after which training is stopped regardless of number of epochs\n",
    "            or validation metric. Defaults to 3600*8.0.\n",
    "        hidden_size_range (Tuple[int, int], optional): Minimum and maximum of ``hidden_size`` hyperparameter. Defaults\n",
    "            to (16, 265).\n",
    "        hidden_continuous_size_range (Tuple[int, int], optional):  Minimum and maximum of ``hidden_continuous_size``\n",
    "            hyperparameter. Defaults to (8, 64).\n",
    "        attention_head_size_range (Tuple[int, int], optional):  Minimum and maximum of ``attention_head_size``\n",
    "            hyperparameter. Defaults to (1, 4).\n",
    "        dropout_range (Tuple[float, float], optional):  Minimum and maximum of ``dropout`` hyperparameter. Defaults to\n",
    "            (0.1, 0.3).\n",
    "        learning_rate_range (Tuple[float, float], optional): Learning rate range. Defaults to (1e-5, 1.0).\n",
    "        use_learning_rate_finder (bool): If to use learning rate finder or optimize as part of hyperparameters.\n",
    "            Defaults to True.\n",
    "        trainer_kwargs (Dict[str, Any], optional): Additional arguments to the\n",
    "            `PyTorch Lightning trainer <https://pytorch-lightning.readthedocs.io/en/latest/trainer.html>`_ such\n",
    "            as ``limit_train_batches``. Defaults to {}.\n",
    "        log_dir (str, optional): Folder into which to log results for tensorboard. Defaults to \"lightning_logs\".\n",
    "        study (optuna.Study, optional): study to resume. Will create new study by default.\n",
    "        verbose (Union[int, bool]): level of verbosity.\n",
    "            * None: no change in verbosity level (equivalent to verbose=1 by optuna-set default).\n",
    "            * 0 or False: log only warnings.\n",
    "            * 1 or True: log pruning events.\n",
    "            * 2: optuna logging level at debug level.\n",
    "            Defaults to None.\n",
    "        pruner (optuna.pruners.BasePruner, optional): The optuna pruner to use.\n",
    "            Defaults to optuna.pruners.SuccessiveHalvingPruner().\n",
    "\n",
    "        **kwargs: Additional arguments for the :py:class:`~TemporalFusionTransformer`.\n",
    "\n",
    "    Returns:\n",
    "        optuna.Study: optuna study results\n",
    "    \"\"\"\n",
    "    assert isinstance(train_dataloaders.dataset, TimeSeriesDataSet) and isinstance(\n",
    "        val_dataloaders.dataset, TimeSeriesDataSet\n",
    "    ), \"dataloaders must be built from timeseriesdataset\"\n",
    "\n",
    "    logging_level = {\n",
    "        None: optuna.logging.get_verbosity(),\n",
    "        0: optuna.logging.WARNING,\n",
    "        1: optuna.logging.INFO,\n",
    "        2: optuna.logging.DEBUG,\n",
    "    }\n",
    "    optuna_verbose = logging_level[verbose]\n",
    "    optuna.logging.set_verbosity(optuna_verbose)\n",
    "\n",
    "    loss = kwargs.get(\n",
    "        \"loss\", QuantileLoss()\n",
    "    )  # need a deepcopy of loss as it will otherwise propagate from one trial to the next\n",
    "\n",
    "    # create objective function\n",
    "    def objective(trial: optuna.Trial) -> float:\n",
    "        # Filenames for each trial must be made unique in order to access each checkpoint.\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=os.path.join(model_path, \"trial_{}\".format(trial.number)), filename=\"{epoch}\", monitor=\"val_loss\"\n",
    "        )\n",
    "\n",
    "        learning_rate_callback = LearningRateMonitor()\n",
    "        logger = TensorBoardLogger(log_dir, name=\"optuna\", version=trial.number)\n",
    "        gradient_clip_val = trial.suggest_loguniform(\"gradient_clip_val\", *gradient_clip_val_range)\n",
    "        default_trainer_kwargs = dict(\n",
    "            accelerator=\"auto\",\n",
    "            max_epochs=max_epochs,\n",
    "            gradient_clip_val=gradient_clip_val,\n",
    "            callbacks=[\n",
    "                learning_rate_callback,\n",
    "                checkpoint_callback,\n",
    "                PyTorchLightningPruningCallbackAdjusted(trial, monitor=\"val_loss\"),\n",
    "            ],\n",
    "            logger=logger,\n",
    "            enable_progress_bar=optuna_verbose < optuna.logging.INFO,\n",
    "            enable_model_summary=[False, True][optuna_verbose < optuna.logging.INFO],\n",
    "        )\n",
    "        default_trainer_kwargs.update(trainer_kwargs)\n",
    "        trainer = pl.Trainer(\n",
    "            **default_trainer_kwargs,\n",
    "        )\n",
    "\n",
    "        # create model\n",
    "        hidden_size = trial.suggest_int(\"hidden_size\", *hidden_size_range, log=True)\n",
    "        kwargs[\"loss\"] = copy.deepcopy(loss)\n",
    "        model = TemporalFusionTransformer.from_dataset(\n",
    "            train_dataloaders.dataset,\n",
    "            dropout=trial.suggest_uniform(\"dropout\", *dropout_range),\n",
    "            hidden_size=hidden_size,\n",
    "            hidden_continuous_size=trial.suggest_int(\n",
    "                \"hidden_continuous_size\",\n",
    "                hidden_continuous_size_range[0],\n",
    "                min(hidden_continuous_size_range[1], hidden_size),\n",
    "                log=True,\n",
    "            ),\n",
    "            attention_head_size=trial.suggest_int(\"attention_head_size\", *attention_head_size_range),\n",
    "            log_interval=-1,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # find good learning rate\n",
    "        if use_learning_rate_finder:\n",
    "            lr_trainer = pl.Trainer(\n",
    "                gradient_clip_val=gradient_clip_val,\n",
    "                accelerator=\"auto\",\n",
    "                logger=False,\n",
    "                enable_progress_bar=False,\n",
    "                enable_model_summary=False,\n",
    "            )\n",
    "            tuner = Tuner(lr_trainer)\n",
    "            res = tuner.lr_find(\n",
    "                model,\n",
    "                train_dataloaders=train_dataloaders,\n",
    "                val_dataloaders=val_dataloaders,\n",
    "                early_stop_threshold=10000,\n",
    "                min_lr=learning_rate_range[0],\n",
    "                num_training=100,\n",
    "                max_lr=learning_rate_range[1],\n",
    "            )\n",
    "\n",
    "            loss_finite = np.isfinite(res.results[\"loss\"])\n",
    "            if loss_finite.sum() > 3:  # at least 3 valid values required for learning rate finder\n",
    "                lr_smoothed, loss_smoothed = sm.nonparametric.lowess(\n",
    "                    np.asarray(res.results[\"loss\"])[loss_finite],\n",
    "                    np.asarray(res.results[\"lr\"])[loss_finite],\n",
    "                    frac=1.0 / 10.0,\n",
    "                )[min(loss_finite.sum() - 3, 10) : -1].T\n",
    "                optimal_idx = np.gradient(loss_smoothed).argmin()\n",
    "                optimal_lr = lr_smoothed[optimal_idx]\n",
    "            else:\n",
    "                optimal_idx = np.asarray(res.results[\"loss\"]).argmin()\n",
    "                optimal_lr = res.results[\"lr\"][optimal_idx]\n",
    "            optuna_logger.info(f\"Using learning rate of {optimal_lr:.3g}\")\n",
    "            # add learning rate artificially\n",
    "            model.hparams.learning_rate = trial.suggest_uniform(\"learning_rate\", optimal_lr, optimal_lr)\n",
    "        else:\n",
    "            model.hparams.learning_rate = trial.suggest_loguniform(\"learning_rate\", *learning_rate_range)\n",
    "\n",
    "        # fit\n",
    "        trainer.fit(model, train_dataloaders=train_dataloaders, val_dataloaders=val_dataloaders)\n",
    "\n",
    "        # report result\n",
    "        return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "    # setup optuna and run\n",
    "    if study is None:\n",
    "        study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "    return study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.data.examples import get_stallion_data\n",
    "data = get_stallion_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['date'] == '2013-01-01']['timeseries'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add time index\n",
    "data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
    "data[\"time_idx\"] -= data[\"time_idx\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add additional features\n",
    "data[\"month\"] = data.date.dt.month.astype(str).astype(\"category\")  # categories have be strings\n",
    "data[\"log_volume\"] = np.log(data.volume + 1e-8)\n",
    "data[\"avg_volume_by_sku\"] = data.groupby([\"time_idx\", \"sku\"], observed=True).volume.transform(\"mean\") # sku별 평균\n",
    "data[\"avg_volume_by_agency\"] = data.groupby([\"time_idx\", \"agency\"], observed=True).volume.transform(\"mean\") # agency별 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to encode special days as one variable and thus need to first reverse one-hot encoding\n",
    "special_days = [\n",
    "    \"easter_day\",\n",
    "    \"good_friday\",\n",
    "    \"new_year\",\n",
    "    \"christmas\",\n",
    "    \"labor_day\",\n",
    "    \"independence_day\",\n",
    "    \"revolution_day_memorial\",\n",
    "    \"regional_games\",\n",
    "    \"fifa_u_17_world_cup\",\n",
    "    \"football_gold_cup\",\n",
    "    \"beer_capital\",\n",
    "    \"music_fest\",\n",
    "]\n",
    "data[special_days] = data[special_days].apply(lambda x: x.map({0: \"-\", 1: x.name})).astype(\"category\")\n",
    "data.sample(10, random_state=521)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the last six months as a validation set\n",
    "max_prediction_length = 6\n",
    "max_encoder_length = 24\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "# training_cutoff: validation set은 마지막 6개월만 => 59 - (max_prediction_length)\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"volume\",\n",
    "    group_ids=[\"agency\", \"sku\"],    #  list of column names identifying a time series\n",
    "    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"agency\", \"sku\"],  # cat. variables that do not change over time\n",
    "    static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],   # cont. variables that do not change over time\n",
    "    time_varying_known_categoricals=[\"special_days\", \"month\"],  # cat. variables that change over time & are known in the future\n",
    "    variable_groups={\"special_days\": special_days},  # group of categorical variables can be treated as one variable\n",
    "    time_varying_known_reals=[\"time_idx\", \"price_regular\", \"discount_in_percent\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "        \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ],  # cont. variables that change over time & are not known in the future (TARGET)\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"agency\", \"sku\"], transformation=\"softplus\"\n",
    "    ),  # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last (max_prediction_length) points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "# TimeSeriesDataSet.from_dataset: Generate dataset with different underlying data but same variable encoders and scalers, etc.\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 128  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huigyu\\anaconda3\\envs\\BSS\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:199: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "c:\\Users\\huigyu\\anaconda3\\envs\\BSS\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:199: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "c:\\Users\\huigyu\\anaconda3\\envs\\BSS\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Missing logger folder: c:\\Users\\huigyu\\workspace\\BikeSharingSystems_rnd\\lightning_logs\n",
      "c:\\Users\\huigyu\\anaconda3\\envs\\BSS\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(293.0088)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n",
    "MAE()(baseline_predictions.output, baseline_predictions.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BSS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
